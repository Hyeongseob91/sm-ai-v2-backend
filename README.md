# SoundMind-AI-V2 : SM-ai-v2

**Multi-Modal AI Agent System** integrating **LLM**, **RAG**, and **MCP (Model Context Protocol)**.
This backend serves as the intelligence layer, capable of autonomous decision-making, internal knowledge retrieval, and external tool usage.

---

## 1. ğŸš€ ì‹¤í–‰ ë°©ë²• (Getting Started)

### í•„ìˆ˜ ìš”êµ¬ ì‚¬í•­
- Python 3.11+
- Node.js (for local MCP server testing)
- `uv` (Python package manager)

### ì„¤ì¹˜ ë° ì‹¤í–‰

#### 1. í™˜ê²½ ì„¤ì •
`.env` íŒŒì¼ì„ ìƒì„±í•˜ê³  API Keyë¥¼ ì…ë ¥í•©ë‹ˆë‹¤.
```ini
OPENAI_API_KEY=sk-...
CHROMA_DB_PATH=./chroma_db
MCP_SERVER_URLS=["http://localhost:8001/sse"]
```

#### 2. ì˜ì¡´ì„± ì„¤ì¹˜
```bash
uv sync
```

#### 3. ì„œë²„ ì‹¤í–‰
**Backend Server (FastAPI)**
```bash
uv run uvicorn src.api.main:app --reload
```

**Test MCP Server (Optional)**
```bash
uv run uvicorn tools.mcp_server:app --port 8001
```

#### 4. API ë¬¸ì„œ í™•ì¸
ë¸Œë¼ìš°ì €ì—ì„œ [http://127.0.0.1:8000/docs](http://127.0.0.1:8000/docs) ì ‘ì†.

---

## 2. ğŸ—ï¸ ì„¤ê³„ ê°œìš” ë° êµ¬ì¡° (Architecture)

ì´ í”„ë¡œì íŠ¸ëŠ” **Clean Architecture**ì™€ **Layered Architecture** ì›ì¹™ì„ ë”°ë¦…ë‹ˆë‹¤.

```mermaid
graph TD
    Client["Client / Frontend"] -->|HTTP POST| API["API Layer (FastAPI)"]
    
    subgraph "Backend System"
        API -->|Invoke| ChatSystem["Chat System (LangGraph)"]
        API -->|Upload| RAGSystem["RAG System"]
        
        subgraph "Core Layer"
            LLM["LLM Service"]
            MCP["MCP Manager"]
            Session["Session Manager"]
        end
        
        ChatSystem -->|Use| LLM
        ChatSystem -->|Use| MCP
        ChatSystem -->|Use| RAGSystem
        
        RAGSystem -->|Store/Query| VectorDB[("VectorDB (ChromaDB)")]
        MCP -->|Connect| ExternalTools["External MCP Servers"]
    end
```

---

## 3. ğŸ“‚ í´ë” ë° íŒŒì¼ ì—­í•  (Directory Structure)

| ê²½ë¡œ | ì—­í•  ë° ì„¤ëª… |
| :--- | :--- |
| **`src/api/`** | **ì™¸ë¶€ ì¸í„°í˜ì´ìŠ¤ ê³„ì¸µ.** HTTP ìš”ì²­ì„ ë°›ì•„ ë¹„ì¦ˆë‹ˆìŠ¤ ë¡œì§ìœ¼ë¡œ ì „ë‹¬í•©ë‹ˆë‹¤. |
| `â”œâ”€ main.py` | FastAPI ì•± ì§„ì…ì . ìˆ˜ëª… ì£¼ê¸°(Startup/Shutdown) ê´€ë¦¬. |
| `â”œâ”€ router.py` | ëª¨ë“  API ë¼ìš°í„°ë¥¼ í†µí•© ê´€ë¦¬. |
| `â”œâ”€ chat_endpoints.py` | `/chat` ì—”ë“œí¬ì¸íŠ¸. LangGraph ì‹¤í–‰ ìš”ì²­ ì²˜ë¦¬. |
| `â””â”€ rag_endpoints.py` | `/rag/ingest` ì—”ë“œí¬ì¸íŠ¸. ë¬¸ì„œ ì—…ë¡œë“œ ì²˜ë¦¬. |
| **`src/core/`** | **í•µì‹¬ ì¸í”„ë¼ ê³„ì¸µ.** ì‹œìŠ¤í…œ ì „ë°˜ì—ì„œ ì‚¬ìš©ë˜ëŠ” ê³µí†µ ì„œë¹„ìŠ¤. |
| `â”œâ”€ llm_service.py` | OpenAI/Anthropic ë“± LLM í´ë¼ì´ì–¸íŠ¸ íŒ©í† ë¦¬. |
| `â”œâ”€ mcp_manager.py` | ì™¸ë¶€ MCP ì„œë²„ì™€ì˜ ì—°ê²° ë° ë„êµ¬ ë¡œë“œ ê´€ë¦¬. |
| `â”œâ”€ mcp_client.py` | ì‹¤ì œ SSE í†µì‹ ì„ ë‹´ë‹¹í•˜ëŠ” MCP í´ë¼ì´ì–¸íŠ¸ êµ¬í˜„ì²´. |
| `â””â”€ session_manager.py` | ëŒ€í™” ìƒíƒœ(State) ì €ì¥ì„ ìœ„í•œ Checkpointer ê´€ë¦¬. |
| **`src/systems/`** | **ë¹„ì¦ˆë‹ˆìŠ¤ ë¡œì§ ê³„ì¸µ.** êµ¬ì²´ì ì¸ ê¸°ëŠ¥ êµ¬í˜„ì²´. |
| **`â”œâ”€ chat/`** | **Agentic Chat System** |
| `â”‚  â”œâ”€ graph.py` | LangGraph ì •ì˜ (StateGraph êµ¬ì„±). |
| `â”‚  â”œâ”€ agent.py` | Agent ë…¸ë“œ ë¡œì§ (LLM í˜¸ì¶œ ë° íŒë‹¨). |
| `â”‚  â””â”€ tools.py` | Agentê°€ ì‚¬ìš©í•  ë„êµ¬(RAG + MCP) ë°”ì¸ë”©. |
| **`â””â”€ rag/`** | **RAG (Retrieval-Augmented Generation) System** |
| `   â”œâ”€ ingestion.py` | ë¬¸ì„œ ë¡œë“œ, ì²­í‚¹(Splitting), ì„ë² ë”© ì²˜ë¦¬. |
| `   â”œâ”€ vector_store.py` | ChromaDB ì‹±ê¸€í†¤ ë˜í¼. |
| `   â””â”€ tool.py` | Agentê°€ ê²€ìƒ‰ì„ ìˆ˜í–‰í•  ë•Œ ì‚¬ìš©í•˜ëŠ” `BaseTool` ë˜í¼. |

---

## 4. âš™ï¸ ë¹„ì¦ˆë‹ˆìŠ¤ ë¡œì§ ì²˜ë¦¬ ìˆœì„œ (Detailed Flows)

### A. ì±„íŒ… ë° ì—ì´ì „íŠ¸ ì‹¤í–‰ íë¦„ (`POST /v1/chat`)

ì‚¬ìš©ìê°€ ë©”ì‹œì§€ë¥¼ ë³´ë‚´ë©´ **LangGraph**ê°€ ë£¨í”„ë¥¼ ëŒë©° ë‹µë³€ì„ ìƒì„±í•©ë‹ˆë‹¤.

```mermaid
sequenceDiagram
    participant User
    participant API as API (chat_endpoints)
    participant Graph as LangGraph (graph.py)
    participant Agent as Agent Node (agent.py)
    participant LLM as LLM Service
    participant Tools as Tool Node
    participant RAG as RAG Tool
    participant MCP as MCP Tool

    User->>API: POST /chat (Message)
    API->>Graph: app.ainvoke(messages)
    
    loop Agent Loop
        Graph->>Agent: call_model(state)
        Agent->>LLM: bind_tools() & invoke()
        LLM-->>Agent: Response (Text OR ToolCall)
        
        alt is Tool Call?
            Agent->>Tools: Execute Tool
            alt RAG Search
                Tools->>RAG: search_knowledge_base(query)
                RAG-->>Tools: Document Content
            else MCP Action
                Tools->>MCP: call_external_tool(args)
                MCP-->>Tools: Tool Result
            end
            Tools-->>Graph: Update State (ToolMessage)
            Note right of Graph: Loop back to Agent
        else is Final Answer?
            Agent-->>Graph: Final Response
            Note right of Graph: End Loop
        end
    end
    
    Graph-->>API: Final State
    API-->>User: ChatResponse
```

**ìƒì„¸ í•¨ìˆ˜ í˜¸ì¶œ ìˆœì„œ:**
1.  `src.api.chat_endpoints.chat_endpoint()`: ìš”ì²­ ìˆ˜ì‹ .
2.  `src.systems.chat.graph.get_graph()`: ì»´íŒŒì¼ëœ LangGraph ì•± ê°€ì ¸ì˜¤ê¸°.
3.  `app.ainvoke()`: ê·¸ë˜í”„ ì‹¤í–‰ ì‹œì‘.
4.  **Node: Agent** (`src.systems.chat.agent.call_model`)
    *   `LLMService.get_llm()`: LLM ì¸ìŠ¤í„´ìŠ¤ íšë“.
    *   `src.systems.chat.tools.get_all_tools()`: RAG ë° MCP ë„êµ¬ ëª©ë¡ ë¡œë“œ.
    *   `llm.bind_tools(tools)`: ë„êµ¬ ë°”ì¸ë”©.
    *   `llm.ainvoke()`: LLMì—ê²Œ ì§ˆë¬¸ ì „ë‹¬.
5.  **Edge: Conditional** (`should_continue`)
    *   LLM ì‘ë‹µì— `tool_calls`ê°€ ìˆìœ¼ë©´ -> `tools` ë…¸ë“œë¡œ ì´ë™.
    *   ì—†ìœ¼ë©´ -> `END`ë¡œ ì´ë™.
6.  **Node: Tools** (`ToolNode`)
    *   `tool_calls`ì— ëª…ì‹œëœ ë„êµ¬ ì‹¤í–‰ (ì˜ˆ: `RAGTool._run` ë˜ëŠ” `MCPClient.session.call_tool`).
    *   ê²°ê³¼ë¥¼ `ToolMessage`ë¡œ ìƒíƒœì— ì¶”ê°€.
7.  **Loop**: ë‹¤ì‹œ **Agent** ë…¸ë“œë¡œ ëŒì•„ê°€ì„œ ë„êµ¬ ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ ìµœì¢… ë‹µë³€ ìƒì„±.

---

### B. RAG ë¬¸ì„œ ì—…ë¡œë“œ íë¦„ (`POST /v1/rag/ingest`)

ë¬¸ì„œë¥¼ ì—…ë¡œë“œí•˜ì—¬ ë²¡í„° DBì— ì €ì¥í•˜ëŠ” ê³¼ì •ì…ë‹ˆë‹¤.

```mermaid
sequenceDiagram
    participant User
    participant API as API (rag_endpoints)
    participant Service as IngestionService
    participant Splitter as TextSplitter
    participant VectorDB as VectorStore (Chroma)

    User->>API: POST /rag/ingest (File)
    API->>Service: process_file(UploadFile)
    Service->>Service: Save temp file
    Service->>Service: Load (PyPDF/TextLoader)
    Service->>Splitter: split_documents()
    Splitter-->>Service: List[Chunk]
    Service->>VectorDB: add_documents(chunks)
    VectorDB-->>Service: Success
    Service-->>API: Result (chunks_count)
    API-->>User: BaseResponse
```

**ìƒì„¸ í•¨ìˆ˜ í˜¸ì¶œ ìˆœì„œ:**
1.  `src.api.rag_endpoints.ingest_document()`: íŒŒì¼ ìˆ˜ì‹ .
2.  `src.systems.rag.ingestion.IngestionService.process_file()`: ë©”ì¸ ë¡œì§ ì‹¤í–‰.
3.  `_load_file()`: íŒŒì¼ í™•ì¥ìì— ë”°ë¼ `PyPDFLoader` ë˜ëŠ” `TextLoader` ì„ íƒ í›„ ë¡œë“œ.
4.  `RecursiveCharacterTextSplitter.split_documents()`: ê¸´ í…ìŠ¤íŠ¸ë¥¼ ì²­í¬ ë‹¨ìœ„ë¡œ ë¶„í• .
5.  `src.systems.rag.vector_store.VectorStore.add_documents()`:
    *   `OpenAIEmbeddings`: í…ìŠ¤íŠ¸ë¥¼ ë²¡í„°ë¡œ ë³€í™˜.
    *   `Chroma.add_documents()`: ë²¡í„° DBì— ì €ì¥ ë° ì¸ë±ì‹±.
